{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b9701d-810a-446a-8c69-fc481c47ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8c08fa-fcf6-4213-957a-07b55019a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images into numoy arrays\n",
    "def load_image(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return np.asarray(img).astype(float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997d80fb-3891-4d30-a043-3a2d5a9c2bc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.29 GiB for an array with shape (4828, 320, 240, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Target_vars[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m     19\u001b[0m         genretated_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m2\u001b[39m)    \n\u001b[1;32m---> 21\u001b[0m images_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(images_list)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.29 GiB for an array with shape (4828, 320, 240, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "Target_vars = ['NotConnected', 'Stable', 'unstable']\n",
    "genretated_labels = []\n",
    "\n",
    "# Initialize an empty list to store the images\n",
    "images_list = []\n",
    "image_directory = 'C:/Users/harsi/Documents/allegro-research/Images_CNN'\n",
    "\n",
    "# genrating labels for images and storing the images in a numpy array\n",
    "for file in os.listdir(image_directory):\n",
    "    img_path = os.path.join(image_directory, file)\n",
    "    img = load_image(img_path)\n",
    "    images_list.append(img)\n",
    "    \n",
    "    if Target_vars[0] in file:\n",
    "        genretated_labels.append(0)\n",
    "    if Target_vars[1] in file:\n",
    "        genretated_labels.append(1)              \n",
    "    if Target_vars[2] in file:\n",
    "        genretated_labels.append(2)    \n",
    "        \n",
    "images_array = np.array(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a4195bbd-ef3d-4a96-8850-7f06d0730176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing labels to a file\n",
    "with open('labels.txt', 'w') as file:\n",
    "    for label in genretated_labels:\n",
    "        file.write(f\"{label}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff001d56-9a4f-4384-8894-fe622671c8be",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "15a005c7-33c1-4382-af3d-93d90bfb45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "#layer 1\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(320, 240, 3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#layer 2\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#layer 3\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# compiling model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af663f-e570-49c1-af32-82f4d6613265",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "845aaf61-8ca6-4737-8caf-f7d05f88e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = images_array\n",
    "y = np.array(genretated_labels)\n",
    "\n",
    "# Split the data: 80% for training and 20% for testing\n",
    "training_images, testing_images, training_labels, testing_labels = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeb24b-917a-4648-b6d1-872596f9a874",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "39481a98-5da9-4a40-a2ef-426957d96e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 658ms/step - accuracy: 0.4888 - loss: 0.7397 - val_accuracy: 1.0000 - val_loss: 0.1534\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 578ms/step - accuracy: 0.9760 - loss: 0.2292 - val_accuracy: 1.0000 - val_loss: 0.0189\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600ms/step - accuracy: 0.9604 - loss: 0.2119 - val_accuracy: 1.0000 - val_loss: 0.0314\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 574ms/step - accuracy: 0.9635 - loss: 0.1878 - val_accuracy: 1.0000 - val_loss: 0.0422\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.9604 - loss: 0.1965 - val_accuracy: 1.0000 - val_loss: 0.0280\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9562 - loss: 0.2166 - val_accuracy: 1.0000 - val_loss: 0.0514\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.9697 - loss: 0.1627 - val_accuracy: 1.0000 - val_loss: 0.0727\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9666 - loss: 0.1852 - val_accuracy: 1.0000 - val_loss: 0.0137\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9749 - loss: 0.1470 - val_accuracy: 1.0000 - val_loss: 0.0285\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.9635 - loss: 0.1851 - val_accuracy: 1.0000 - val_loss: 0.1453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b683ed990>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model\n",
    "model.fit(training_images, training_labels, epochs=5, validation_data=(testing_images, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b8ae1ce6-1c6c-4a58-ab0a-be9657e7acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 1.0000 - loss: 0.1453\n",
      "Loss: 0.1452813297510147\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#testing model\n",
    "loss, accuracy = model.evaluate(testing_images, testing_labels)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5152a35-b515-42c3-8777-a8c27b936ea8",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0e7283d4-8335-453c-b85b-04e544daf3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testing_images)\n",
    "indexes = [np.argmax(prediction) for prediction in predictions]\n",
    "indexes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
